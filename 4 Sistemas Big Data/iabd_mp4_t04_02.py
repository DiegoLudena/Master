# -*- coding: utf-8 -*-
"""IABD_MP4_T04_02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hr9xQ2uLEqJWeCdrcQzhJtq_8C15KGC5

EJERCICIO 2: Crea una base de datos en PySpark que contenga información de 10 personas, incluyendo su nombre, edad, peso y talla. Luego, filtra la base de datos para encontrar a las personas que tienen un peso superior a 70 kg y una talla inferior a 180 cm.
Requisitos:
•	Utiliza el framework PySpark.
•	Crea un DataFrame con las columnas "Nombre", "Edad", "Peso" y "Talla".
•	Define los tipos de datos de cada columna (StringType, IntegerType).
•	Introduce información de 10 personas ficticias en el DataFrame.
•	Aplica un filtro al DataFrame para seleccionar las personas que cumplen con los criterios de peso y talla.
•	Muestra el resultado del filtrado en la consola.

Para empezar a trabar con PySpark, lo primero es instalar las dependencias necesarias, configurar las variables de entorno e iniciar la sesión de Spark.
"""

# Instalamos las dependencias necesarias si no las tenemos (descomentar):
# !apt-get install openjdk-8-jdk-headless -qq > /dev/null
# !wget -q https://downloads.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz
# !tar xf spark-3.5.4-bin-hadoop3.tgz
# !pip install -q findspark

# Configuramos las variables de entorno si es necesario (descomentar):
# import os
# os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
# os.environ["SPARK_HOME"] = "/content/spark-3.5.4-bin-hadoop3"

# Iniciamos la sesión de Spark:
import findspark
findspark.init()

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
from pyspark.sql.functions import avg
from pyspark.sql.functions import count, floor, concat, lit, min, max, min, max, first, last


spark = SparkSession.builder.appName("MiAplicacion").getOrCreate()

"""Creamos el dataframe que pide el enunciado con 10 personas, nombre, edad, peso y talla."""

# Define el esquema del DataFrame
schema = StructType([
    StructField("Nombre", StringType(), True),
    StructField("Edad", IntegerType(), True),
    StructField("Peso", IntegerType(), True),
    StructField("Talla", IntegerType(), True)
])

# Crea una lista con los datos de las personas
data = [
    ("Alicia", 25, 60, 165),
    ("Benito", 30, 75, 180),
    ("Carolina", 35, 65, 170),
    ("Francisco", 40, 80, 185),
    ("Diana", 22, 55, 160),
    ("Elena", 33, 70, 175),
    ("Fernando", 28, 62, 168),
    ("Gabriela", 38, 85, 190),
    ("Héctor", 26, 58, 162),
    ("Isabel", 31, 72, 178)
]

# Crea el DataFrame
df = spark.createDataFrame(data, schema)

# Muestra el DataFrame (para comprobar que se ha creado correctamente)
df.show()

"""Con el dataframe creado, hacemos el filtro que seleccione aquellas que tienen un peso superior a 70 kg y una talla inferior a 180 cm"""

# Aplica el filtro al DataFrame
filtered_df = df.filter((df["Peso"] > 70) & (df["Talla"] < 180))

# Muestra el resultado del filtrado
filtered_df.show()

"""Con esto estaría el ejercicio. Por hacer alguna cosa más de práctica con PySpark vamos a incluir algunos cálculos estadísticos, agregación y ordenación."""

# Calcula las medias de Edad, Peso y Talla
avg_edad = df.select(avg("Edad")).collect()[0][0]
avg_peso = df.select(avg("Peso")).collect()[0][0]
avg_talla = df.select(avg("Talla")).collect()[0][0]

# Imprime los resultados
print(f"La media de la edad es: {avg_edad}")
print(f"La media del peso es: {avg_peso}")
print(f"La media de la talla es: {avg_talla}")

# Agrupa por edad y cuenta las personas
edad_counts = df.groupBy("Edad").agg(count("*").alias("NumPersonas")).orderBy("Edad")

# Muestra el resultado
edad_counts.show()

"""Como contar de uno en uno es un poco tonto, creamos intervalos."""

# Crea una nueva columna con el intervalo de edad
#floor(df["Edad"] / 5) * 5: El número del floor, que representa el inicio del intervalo.
# La función lit() se utiliza para crear un valor literal a partir de una cadena,
#floor(df["Edad"] / 5) * 5 + 4: El número del floor más 4, que representa el final del intervalo.
# La función concat() une las tres partes
df = df.withColumn("IntervaloEdad", concat(floor(df["Edad"] / 5) * 5, lit("-"), floor(df["Edad"] / 5) * 5 + 4))

# Agrupa por intervalo de edad y cuenta las personas
edad_counts = df.groupBy("IntervaloEdad").agg(count("*").alias("NumPersonas")).orderBy("IntervaloEdad")

# Muestra el resultado
edad_counts.show()

# Calcula la talla máxima y mínima
talla_maxima = df.select(max("Talla")).collect()[0][0] #max busca el valor máximo de la columna
talla_minima = df.select(min("Talla")).collect()[0][0 ]#min busca el valor mínimo de la columna

# Imprime los resultados
print(f"La talla máxima es: {talla_maxima}")
print(f"La talla mínima es: {talla_minima}")

# Ordena por peso de forma descendente
df_ordenado = df.orderBy("Peso", ascending=False)

# Obtén la persona con más peso y su peso
# con first selecciona el primer elemento de la lista ordenada
persona_mas_peso = df_ordenado.select(first("Nombre"), first("Peso")).first()

# Obtén la persona con menos peso y su peso
# con last selecciona el último elemento de la lista ordenada
persona_menos_peso = df_ordenado.select(last("Nombre"), last("Peso")).first()


# Imprime los resultados
print(f"La persona con más peso es {persona_mas_peso[0]}, que pesa {persona_mas_peso[1]} Kg")
print(f"La persona con menos peso es {persona_menos_peso[0]}, que pesa {persona_menos_peso[1]} Kg")

# Muestra el resultado sin mostrar la columna IntervaloEdad, que creamos para hacer cálculos
df_ordenado.select("Nombre", "Edad", "Peso", "Talla").show()
# -*- coding: utf-8 -*-
"""IABD_MP4_T04_06.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1us3wD_QkHLfz5x0Ggf1gxVmKOVvtKRaU

EJERCICIO 6: Desarrolla una aplicación en PySpark que utilice el modelo MapReduce para procesar un conjunto de frases relacionadas con Big Data. El objetivo es contar cuántas veces aparece cada palabra en el dataset de frases que deberás crear tú mismo.
Requisitos:
1.	Crea un dataset de al menos 10 frases relacionadas con temas técnicos como Big Data, MapReduce, y PySpark.
2.	Aplica el modelo MapReduce en PySpark siguiendo estas fases:
o	Map: Divide las frases en palabras y asigna un valor 1 a cada palabra.
o	Shuffle: Agrupa todas las palabras iguales.
o	Reduce: Suma las ocurrencias de cada palabra.
3.	El resultado debe mostrar la frecuencia de cada palabra en el conjunto de frases.
Entrega:
•	Presenta el código que has desarrollado para implementar el modelo MapReduce.
•	Muestra los resultados obtenidos en la frecuencia de palabras y proporciona una breve explicación de las fases del proceso.

Para empezar a trabar con PySpark, lo primero es instalar las dependencias necesarias, configurar las variables de entorno e iniciar la sesión de Spark.
"""

# Instalamos las dependencias necesarias si no las tenemos (descomentar):
# !apt-get update
# !apt-get install openjdk-8-jdk-headless -qq > /dev/null
# !wget -q https://downloads.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz
# !tar xf spark-3.5.4-bin-hadoop3.tgz
# !pip install -q findspark

# Configuramos las variables de entorno si es necesario (descomentar):
# import os
# os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
# os.environ["SPARK_HOME"] = "/content/spark-3.5.4-bin-hadoop3"

# Iniciamos la sesión de Spark:
import findspark
findspark.init()

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType
# Importar la función explode para dividir las frases en palabras
from pyspark.sql.functions import explode, split, lower, lit, sum, collect_list, regexp_replace, desc



spark = SparkSession.builder.appName("MiAplicacion").getOrCreate()

"""Después creamos la lista de frases, definimos el esquema del dataset y creamos el RDD (Resilient Distributed Dataset) a partir de la lista de frases y, por último creamos el datagrame a partir del RDD y el esquema.
En este ejercicio hemos usado un RDD en lugar de crearlo directamente a partir de la lista de datos, como en el ejercicio 2, porque tiene algunas ventajas: RDD es una colección de datos distribuida en el clúster de Spark, lo que permite procesarlo en paralelo. Al crear un RDD con spark.sparkContext.parallelize, las frases se dividen en particiones y se distribuyen entre los nodos del clúster, lo que acelera el procesamiento. En el caso concreto del ejercicio, el modelo MapReduce, que se basa en transformar y agregar datos distribuidos en un clúster, por lo que las frases se distribuyen en un RDD para paralelizar el procesamiento de las palabras.
Cuando los datos ya están en un formato estructurado, como una lista de tuplas como el ejercicio 2 o un archivo CSV, crear un DataFrame directamente es más sencillo y rápido.
"""

# Crear una lista de frases:
frases = [
    "Big Data es el futuro del análisis de datos.",
    "MapReduce es un modelo de programación para procesar grandes conjuntos de datos.",
    "PySpark es una librería de Python para trabajar con Spark.",
    "Spark es un framework de computación distribuida.",
    "El análisis de datos es crucial para las empresas.",
    "Hadoop es un ecosistema para el almacenamiento y procesamiento de Big Data.",
    "Los datos no estructurados son un desafío para el análisis.",
    "La ciencia de datos es un campo en crecimiento.",
    "El aprendizaje automático se utiliza para extraer información de los datos.",
    "La visualización de datos es importante para comunicar los resultados.",
]

#Definir el esquema del dataset:
esquema = StructType([
    StructField("frase", StringType(), True),
])

# Transformar la lista de frases en una lista de tuplas
# Cada tupla representa una fila en el DataFrame
data = [(frase,) for frase in frases]

#Crear un RDD a partir de la lista de frases:
rdd = spark.sparkContext.parallelize(data)

#Crear un DataFrame a partir del RDD y el esquema:
df = spark.createDataFrame(rdd, esquema)

# Mostrar el DataFrame
df.show()

"""Comenzamos el proceso Map Reduce.
Primero viene la fase Map. En esta fase primero se tokenizan las frases, dividiéndolas en palabras individuales, y se asigna el valor 1 a cada palabra. Con esto ya tenemos los pares clave-valor.
Para evitar que las palabras que acompañan a signos de puntuación sean reconocidas como palabras distintas usamos la funcion regexp_replace  para sustituirlas por un espacio de en blanco antes de tokenizar las frases.
"""

# Tokenizar las frases y crear un nuevo DataFrame con una columna para cada palabra,
# eliminando los signos de puntuación
palabras = df.select(explode(split(regexp_replace(lower(df.frase), "[.,:;?!]", " "), " ")).alias("palabra"))

# Filtrar las palabras vacías
palabras = palabras.filter(palabras.palabra != "")

# Asignar un valor 1 a cada palabra
map = palabras.withColumn("valor", lit(1))

# Mostrar el resultado de la fase Map (primeras 10 filas)
map.show(10)

"""La fase Sort&Shuffle agrupa las palaras iguales, usando la función groupBy. PySpark realiza automáticamente el proceso de Shuffle para distribuir las palabras iguales entre los nodos del clúster."""

# Agrupar las palabras iguales utilizando groupBy, sin aplicar la suma aún
sort = map.groupBy("palabra")

# Mostrar el resultado de la fase Sort&Shuffle (primeras 10 filas)
sort.agg(collect_list("valor")).withColumnRenamed("collect_list(valor)", "valores").show(10, truncate=False)



"""Por último la fase Reduce usa la función sum para obrtener la frecuencia de cada palabra. Para que el resultado no salga por el orden aleatorio que se generó en la fase Shuffle (recordamos que PySpark hace automáticamente este proceso al distribuir la carga en los nodos del clúster) si no en un orden descendente de mayor a menor frecuencia, que entendemos que es más interesante, usamos la funcion desc."""

# Aplicar la función de agregación sum para obtener la frecuencia de cada palabra
reduce = sort.sum("valor").withColumnRenamed("sum(valor)", "frecuencia")

# Ordenar por frecuencia en orden descendente
reduce_ordenado = reduce.orderBy(desc("sum(valor)"))

# Mostrar el resultado ordenado
reduce_ordenado.show()
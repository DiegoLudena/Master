# -*- coding: utf-8 -*-
"""IABD_MP5_T03_01y02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uODtwTCb5_yLI42zM8p-t11ANhmHO9v_

EJERCICIO 1:  Limpieza y Validación de Datos:
•	Seleccionar un dataset de kaggle que os parezca interesante y sea de un tamaño apropiado (que tenga muchas filas y un buen número de columnas).
Podéis usar el dataset Uber.csv que podéis encontrar en kaggle (hay múltiples de ellos en la web).
•	Ejercicios:
o	Identifica y elimina filas duplicadas.
o	Comprueba la integridad de los datos en columnas como el precio, el ID, el nombre, etc. (valida formatos, rangos, etc.).
o	Reemplaza valores nulos o inválidos según las mejores prácticas para el dataset.
o	Normaliza o estandariza algunas columnas numéricas para mejorar el análisis.

Elegimos el dataset Chess Game Dataset (Lichess). Comenzamos por cargar el dataset y explorarlo:
"""

# Instalar ydata profiling si es necesario (descomentar)
!pip install ydata-profiling


import pandas as pd
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from ydata_profiling import ProfileReport
from scipy.stats import chi2_contingency



# Cargar el dataset
df = pd.read_csv('games.csv')

#Visualizar las primeras filas
df.head()

# Obtener información sobre las columnas: nombre, tipo, cantidad de no nulos
df.info()

"""El número de Non-Null Count de todas las columnas coincide con el número de entradas, así que no hay ningún NaN en el dataset.


"""

nan_counts = df.isnull().sum()
print(nan_counts)

"""Vamos a visualizar si hay duplicados:"""

df.duplicated().sum()

"""Verificamos que sean duplicados y no haya ningún error:"""

duplicados = df[df.duplicated(keep=False)]  # keep=False para mostrar todas las filas duplicadas
print(duplicados.head())

"""Efectivamente hay partidas que aparecen duplicadas, procedemos a eliminarlas del dataset:"""

# Eliminamos las filas duplicadas
df = df.drop_duplicates(keep='first')

# Verificamos que se hayan eliminado correctamente
print(df.duplicated().sum())

"""Ya que el factor identificativo de las partidas es el id, y no la primera columna que sería un número de registro, pensamos que podría haber partidas duplicadas pero que al estar en filas distintas no aparecen como duplicadas. Por eso vamos a comprobar si existen ids duplicados"""

df['id'].duplicated().sum()

df_sorted = df.sort_values(by=['id'])
duplicados = df_sorted[df_sorted['id'].duplicated(keep=False)]
print(duplicados)

"""Efectivamente los hay y son partidas duplicadas, que no salieron en df.duplicated por tener la primera columna, el índice, distinto. Hay que eliminarlas también"""

# Eliminamos las filas duplicadas
df.drop_duplicates(subset=['id'], keep='first', inplace=True)

# Verificamos que se hayan eliminado correctamente
print(df['id'].duplicated().sum())

"""Vamos a comprobar que las columnas están en rango.
winner solo puede ser black, white o draw. victory_status solo puede ser resign, mate, outoftime o draw. Rated solo puede ser true o false.
"""

# Inconsistencias en winner:
valores_validos_winner = ['black', 'white', 'draw']
inconsistencias = df[~df['winner'].isin(valores_validos_winner)]
print(inconsistencias)

# Inconsistencias en victory_status:
valores_validos_victory_status = ['resign', 'mate', 'outoftime', 'draw']
inconsistencias = df[~df['victory_status'].isin(valores_validos_victory_status)]
print(inconsistencias)

valores_validos_rated = [True, False]
inconsistencias = df[~df['rated'].isin(valores_validos_rated)]
print(inconsistencias)

"""Las fechas de created_at y last_move_at están en formato Epoch time (cuenta los milisegundos desde el 1 de enero de 1970). Tiene sentido que esté guardado así, pero para poder hacer agregaciones por tiempo es mucho menos intuitivo, así que vamos a cambiar el tipo. Más que cambiarlo, vamos a crear dos columnas nuevas, created_at_datatime y last_move_at_datatime con el nuevo tipo, por si necesitásemos el formato Epoch time para algo."""

# created_at convertido a datetime
df['created_at_datetime'] = pd.to_datetime(df['created_at'], unit='ms')

# last_move_at convertido a datetime
df['last_move_at_datetime'] = pd.to_datetime(df['last_move_at'], unit='ms')

"""Por último, para facilitar los cálculos, vamos a estandarizar las columnas del elo de los jugadores, white_rating y black_rating. Como el ELO ya sigue una distribución normal por cómo se calcula, lo adecuado es estandarizar, no hace falta normalizarlo."""

scaler = StandardScaler()
df[['white_rating_scaled', 'black_rating_scaled']] = scaler.fit_transform(df[['white_rating', 'black_rating']])

"""Con esto consideramos que el dataset ya está preparado para empezar el análisis.

EJERCICIO 2: Profiling y Análisis de Datos:
•	A partir del mismo dataset seleccionado para el ejercicio 1:
o	Utiliza pandas-profiling para obtener un informe detallado del dataset.
o	Analiza las estadísticas descriptivas, las distribuciones de las variables, las correlaciones, etc.
o	Identifica posibles outliers o valores anómalos que puedan afectar el análisis.
o	Determina si hay variables que se pueden transformar para mejorar el análisis.
o	Trata de extraer información útil para el análisis real y trata de conseguir nueva información a partir de los datos.

Aunque estamos trabajando en Google Colab, para que el proyecto pueda utilizarse en otros entornos todas las importaciones se han hecho al comienzo del código, así como la instalación de pandas-profiling si fuese necesaria.
"""

# Generar el reporte de profiling:
profile = ProfileReport(df, title="Pandas Profiling Report", explorative=True)
profile.to_notebook_iframe()

"""Respecto a estadísticas descriptivas, las más interesantes que vemos son victorias, que vemos que la moda es victoria por abandono, el ganador, con una ligera ventaja para las blancas, y los datos de ELO y los turnos que duran las partidas

En la búsqueda de outliers queremos analizar las partidas cortas, para ver si son tablas pactadas, mates muy rápidos como el pastor, o fallos de conexión.
"""

for turns in range(1, 6):
  # Filtrar las partidas con el número de turnos deseado
  filtered_games = df[df['turns'] == turns]

  # Obtener la distribución de victory_status
  victory_status_counts = filtered_games['victory_status'].value_counts()

  # Imprimir los resultados
  print(f"Partidas con {turns} turno(s):")
  print(victory_status_counts)
  print("-" * 20)  # Separador visual

  # Graficar la distribución
  victory_status_counts.plot(kind='bar')
  plt.title(f'Distribución de victory_status en partidas con {turns} turno(s)')
  plt.xlabel('Tipo de victoria')
  plt.ylabel('Frecuencia')
  plt.show()

"""Observamos que con menos de 4 jugadas no puede haber mates, cosa que ya sabíamos por la teoría del ajedrez, y que las partidas perdidas con tan pocos turnos o son pérdidas de tiempo (gente que no se dio cuenta de que había empezado la partida online, muy probablemente) o abandonos... Pero con tan pocos turnos no tiene sentido que sean abandonos por tener la partida perdida y cabe pensar que son desconexiones. Hay algún empate, claramente pactado. Y, por supuesto, los mates del loco y del pastor que aparecen a partir de 4 movimientos sí lo tienen.
La cuestión es que las desconexiones pueden ocurrir en cualquier momento de la partida, y las tablas pactadas, aunque no reflejan una partida real y sería deseable eliminarlas, no ocurren solo en los primeros movimientos, pueden ser después de 10, 15... habría que repasar una a una las partidas, lo que no nos es viable.
Por todo esto, consideramos que las partidas de 1, 2 o 3 turnos, antes de que puedan producirse los primeros mates y pérdidas de piezas mayores, sí podemos descartarlas como partidas sin sentido (outliers), pero no las de más turnos.
"""

# Eliminar partidas con 1, 2 o 3 turnos
df = df[df['turns'] > 3]
# Verificar el número de partidas con 1, 2 o 3 turnos
print(df[df['turns'] <= 3].shape[0])

"""Respecto a los outliers, vamos a comprobar la distribución por años:"""

# Obtener la distribución de la fecha de creación para todas las partidas
year_counts = df['created_at_datetime'].dt.year.value_counts()

# Mostrar los resultados
print("Número de partidas por año:")
print(year_counts)

"""Aunque las partidas en 2013 son válidas igual que cualquier otra y no tiene sentido eliminarlas, tenemos que tener en cuenta para las conclusiones que saquemos por año que son muy pocas y podrían no ser representativas, así que no vamos a tenerlas en cuenta.

Para poder ver la relación entre quién gana y el ELO o el número de turnos sería interesante categorizar primero el status. En realidad esto es interesante solo si queremos hacer cálculos adicionales, en el profilereport hace su propia categorización para calcular sus correlaciones.
"""

mapping_victory_status = {
    'resign': 0,
    'mate': 1,
    'outoftime': 2,
    'draw': 3
}

df['victory_status_num'] = df['victory_status'].map(mapping_victory_status)

"""También es interesante tipificar la columna Winner, en función de si ganó el blanco, el negro o hubo empate. Creamos dos columnas: la primera tendra un 1 si ganaron blancas, la segunda si ganaron negras. El empate mostrará 0 en ambas.

"""

# Crear columnas para victorias de blancas y negras
df['white_win'] = (df['winner'] == 'white').astype(int)
df['black_win'] = (df['winner'] == 'black').astype(int)

"""Vamos a intentar comprobar si efectivamente los jugadores expertos empatan más a menudo, y que cuanto más experto es el jugador es más habitual el abandono antes de terminar la partida con jaque mate."""

# Crear rangos de ELO
bins = [0, 1500, 1800, 2250, float('inf')]
labels = ['Bajo', 'Medio', 'Alto', 'Muy Alto']
df['elo_range'] = pd.cut(df['white_rating'], bins=bins, labels=labels)  # Usando white_rating como ejemplo

# Crear tabla de contingencia
contingency_table = pd.crosstab(df['elo_range'], df['victory_status'])
print(contingency_table)

# Calcular porcentajes por fila (para ver la distribución de victory_status dentro de cada rango de ELO)
percentage_table = contingency_table.div(contingency_table.sum(axis=1), axis=0) * 100
print(percentage_table)

# Diagrama de barras apiladas
contingency_table.plot(kind='bar', stacked=True)
plt.title('Distribución de victory_status por rango de ELO')
plt.xlabel('Rango de ELO')
plt.ylabel('Frecuencia')
plt.show()

# Gráfico de mosaico
sns.heatmap(percentage_table, annot=True, cmap='Blues')
plt.title('Distribución porcentual de victory_status por rango de ELO')
plt.xlabel('victory_status')
plt.ylabel('Rango de ELO')
plt.show()

chi2, p, dof, expected = chi2_contingency(contingency_table)
print(f"Chi-cuadrado: {chi2}")
print(f"Valor p: {p}")

# Si el valor p es menor que un nivel de significancia (e.g., 0.05),
# se rechaza la hipótesis nula de que no hay diferencia en la distribución
# de victory_status entre los rangos de ELO.

"""Se observa claramente las relaciones entre empates y ELO, y abandono y ELO, así como la relación inversa entre mate y ELO. El perder por fin de tiempo sí parece independiente del ELO.

Vamos a intentar ver si hay aperturas con las que es más probable que ganen blancas, negras o que haya empate. Para evitar estudiar cada una de las casi 2000 variantes que tiene el dataset vamos a coger las primeras palabras del nombre de la apertura, seleccionando así el nombre general de la apertura y no cada una de sus variantes.
"""

# Crear una nueva columna con las dos primeras palabras de la apertura, manejando los dos puntos de las variantes
df.loc[:, 'opening_name'] = df['opening_name'].str.split(':').str[0]

# Crear una nueva columna con las dos primeras palabras de la apertura
df.loc[:, 'opening_general'] = df['opening_name'].str.split(' ').apply(lambda x: ' '.join(x[:2]))

# Agrupar por apertura general y contar la frecuencia
opening_counts_general = df.groupby('opening_general')['id'].count().reset_index()
opening_counts_general.rename(columns={'id': 'count'}, inplace=True)

# Ordenar por frecuencia descendente y mostrar las 15 más jugadas
top_15_openings_general = opening_counts_general.sort_values(by='count', ascending=False).head(15)

# Filtrar el dataset por las 15 aperturas más jugadas
filtered_df = df[df['opening_general'].isin(top_15_openings_general['opening_general'])]

# Agrupar por apertura general y resultado, y calcular porcentajes
opening_results = filtered_df.groupby(['opening_general', 'winner'])['id'].count().reset_index()
opening_results.rename(columns={'id': 'count'}, inplace=True)
opening_results['percentage'] = opening_results.groupby('opening_general')['count'].transform(lambda x: x / x.sum() * 100)

# Ajustar el tamaño del gráfico
plt.figure(figsize=(12, 6))

# Crear el gráfico de barras agrupadas con porcentajes
sns.barplot(x='opening_general', y='percentage', hue='winner', data=opening_results, errorbar=None, hue_order=['white', 'black', 'draw'], order=sorted_openings[:15])

# Rotar las etiquetas del eje x para mejor legibilidad
plt.xticks(rotation=90)

# Añadir título y etiquetas de ejes
plt.title('Porcentaje de resultados por apertura (Top 15)')
plt.xlabel('Apertura')
plt.ylabel

"""Observamos que en efecto hay aperturas que ayudan claramente a que ganen las blancas o las negras, y también hay algunas que llevan más a menudo a tablas."""

# Crear un DataFrame para almacenar los resultados
table_data = []

# Obtener las 3 aperturas con mayor porcentaje de victorias para blancas
white_win_top3 = filtered_df.groupby('opening_general')['white_win'].mean().sort_values(ascending=False).head(3)
for opening, percentage in white_win_top3.items():
    table_data.append(['Blancas', opening, f'{percentage * 100:.2f}%'])

# Obtener las 3 aperturas con mayor porcentaje de victorias para negras
black_win_top3 = filtered_df.groupby('opening_general')['black_win'].mean().sort_values(ascending=False).head(3)
for opening, percentage in black_win_top3.items():
    table_data.append(['Negras', opening, f'{percentage * 100:.2f}%'])

# Obtener las 3 aperturas con mayor porcentaje de empates
draw_top3 = filtered_df.groupby('opening_general')[['black_win', 'white_win']].sum()
draw_top3['total_games'] = filtered_df.groupby('opening_general')['id'].count()
draw_top3['draw_percentage'] = (1 - (draw_top3['black_win'] + draw_top3['white_win']) / draw_top3['total_games']) * 100
draw_top3 = draw_top3.sort_values(by='draw_percentage', ascending=False).head(3)

for opening, row in draw_top3.iterrows():
    table_data.append(['Empate', opening, f"{row['draw_percentage']:.2f}%"])



# Crear el DataFrame
results_table = pd.DataFrame(table_data, columns=['Ganador', 'Apertura', 'Porcentaje'])

# Dar formato a la tabla
styled_table = results_table.style.hide(axis='index')

# Mostrar la tabla
styled_table

"""Observamos que las defensas Van't Krujs y la defensa India son claramente ventajosas para las negras, seguidas de cerca por la Siciliana, mientras que las blancas tienen más opciones por encima del 50% de victorias, destacando la Philidor, el gambito de dama y la apertura inglesa. La defensa india, la apertura española (Ruy López) y la Caro-Kan son las más tablíferas en este periodo.

Vamos a estudiar el cambio en la popularidad de las aperturas a lo largo del tiempo. Como dijimos antes, las 53 partidas de 2013 podrían dar una idea equivocada de la popularidad al ser una muestra demasiado pequeña para ser representativa, asi que vamos a comenzar en 2014
"""

# Filtrar el dataset por las 15 aperturas más jugadas
filtered_df = df[(df['opening_general'].isin(top_15_openings_general['opening_general'])) & (df['created_at_datetime'].dt.year != 2013)]

# Agrupar por año y apertura
opening_popularity_by_year = filtered_df.groupby([filtered_df['created_at_datetime'].dt.year, 'opening_general'])['id'].count().reset_index()
opening_popularity_by_year.rename(columns={'id': 'count'}, inplace=True)

# Calcular la frecuencia relativa de cada apertura por año
opening_popularity_by_year['percentage'] = opening_popularity_by_year.groupby('created_at_datetime')['count'].transform(lambda x: x / x.sum() * 100)

# Visualizar la evolución de la popularidad
plt.figure(figsize=(12, 6))
sns.lineplot(x='created_at_datetime', y='percentage', hue='opening_general', data=opening_popularity_by_year)
plt.title('Evolución de la popularidad de las 15 aperturas más jugadas')
plt.xlabel('Año')
plt.ylabel('Porcentaje de partidas')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

# Crear una tabla dinámica con los porcentajes por año y apertura
popularity_table = opening_popularity_by_year.pivot_table(index='opening_general', columns='created_at_datetime', values='percentage', fill_value=0)
# Reducir el número de decimales a dos
popularity_table = popularity_table.astype(float).round(2)

# Añadir una columna con la variación desde el principio al final
popularity_table['Variación'] = popularity_table[popularity_table.columns[-1]] - popularity_table[popularity_table.columns[0]]

# Mostrar la tabla
print(popularity_table)

# Crear el heatmap
plt.figure(figsize=(12, 6))
sns.heatmap(popularity_table.drop(columns=['Variación']), annot=True, cmap='viridis', fmt=".2f")  # Excluir la columna 'Variación' del heatmap
plt.title('Evolución de la popularidad de las 15 aperturas más jugadas')
plt.xlabel('Año')
plt.ylabel('Apertura')
plt.show()

plt.figure(figsize=(12, 6))
popularity_table['color'] = popularity_table['Variación'].apply(lambda x: 'green' if x > 0 else 'red')
sns.barplot(x='Variación', y=popularity_table.index, data=popularity_table, palette=popularity_table['color'].tolist())  # Changed here
plt.title('Variación de la popularidad de las 15 aperturas más jugadas')
plt.xlabel('Variación en el porcentaje de partidas')
plt.ylabel('Apertura')
plt.show()

"""Es llamativo observar cómo en este periodo de tres años la popularidad de la defensa siciliana se dispara, pasando de ser una más a la más utilizada con diferencia. Además de responder a modas podría tener que ver con los rangos de ELO:"""

# Agrupar por rango de ELO y apertura, y contar la frecuencia
opening_counts_by_elo = filtered_df.groupby(['elo_range', 'opening_general'], observed=False)['id'].count().reset_index()
opening_counts_by_elo.rename(columns={'id': 'count'}, inplace=True)

# Calcular la frecuencia relativa de cada apertura por rango de ELO
opening_counts_by_elo['percentage'] = opening_counts_by_elo.groupby('elo_range', observed=False)['count'].transform(lambda x: x / x.sum() * 100)

# Crear el gráfico de barras apiladas
plt.figure(figsize=(12, 6))
sns.barplot(x='elo_range', y='percentage', hue='opening_general', data=opening_counts_by_elo)
plt.title('Distribución de aperturas por rango de ELO')
plt.xlabel('Rango de ELO')
plt.ylabel('Porcentaje de partidas')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

# Crear una tabla para almacenar los resultados
table_data = []

# Iterar sobre los rangos de ELO
for elo_range in labels:
    # Obtener las 3 aperturas más usadas para el rango actual
    top_3_openings = opening_counts_by_elo[opening_counts_by_elo['elo_range'] == elo_range].sort_values(by='percentage', ascending=False).head(3)

    # Agregar la información a la tabla
    for index, row in top_3_openings.iterrows():
        table_data.append([elo_range, row['opening_general'], f"{row['percentage']:.2f}%"])

# Crear el DataFrame
top_3_openings_table = pd.DataFrame(table_data, columns=['Rango de ELO', 'Apertura', 'Porcentaje'])

# Mostrar la tabla
print(top_3_openings_table)

# Calcular la correlación para cada rango de ELO
for elo_range in labels:
    # Filtrar el dataset por el rango de ELO actual
    elo_df = filtered_df[filtered_df['elo_range'] == elo_range]

    # Agrupar por apertura y calcular el porcentaje de victorias
    opening_win_percentage = elo_df.groupby('opening_general')['winner'].apply(lambda x: (x.value_counts(normalize=True) * 100).get('white', 0)).reset_index()
    opening_win_percentage.rename(columns={'winner': 'win_percentage'}, inplace=True)

    # Agrupar por apertura y contar el número de partidas
    opening_popularity = elo_df.groupby('opening_general')['id'].count().reset_index()
    opening_popularity.rename(columns={'id': 'count'}, inplace=True)

    # Combinar los DataFrames
    opening_stats = pd.merge(opening_win_percentage, opening_popularity, on='opening_general')

    # Calcular la correlación
    correlation = opening_stats['win_percentage'].corr(opening_stats['count'])

    # Mostrar el resultado
    print(f"Correlación para el rango de ELO '{elo_range}': {correlation}")

"""Podemos ver que en el nivel de ELO más bajo la distribución es más unifome, y a medida que aumenta el ELO lo hace también la popularidad de la defensa Siciliana, que es muy fuerte para que no pierdan las negras. También vemos que los jugadores de élite han juegan poco la apertura de peón de rey, y probablemente el resto de aficionados la hayan ido abandonando, por imitación, lo que justifica la bajada en su uso con los años."""
# -*- coding: utf-8 -*-
"""IABD_M05_T05_01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H8f9Do22v96NV-nSgi9U1tZQLqqvW8e9

Proyecto KDD: Optimización de Estrategias de Ventas en Amazon (con Pandas)

Utilizar el conjunto de datos de ventas de Amazon y la librería Pandas para realizar un análisis KDD completo, identificar insights valiosos y recomendar estrategias de ventas y marketing para Amazon.thunder
Dataset: "Amazon Sales Dataset" (https://www.kaggle.com/datasets/karkavelrajaj/amazon-sales-dataset/data).

Se debe realizar un seguimiento de todas las fases del modelo KDD para conseguir extraer información útil del dataset.
"""

!pip install nltk

import pandas as pd

import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from collections import Counter

from sklearn.preprocessing import MinMaxScaler

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

"""El primer paso del KDD es la selección de datos. En este caso, los datos ya están seleccionados en el dataset amazon.csv, así que solamente tenemos que cargarlo."""

# Leemos el dataset usando Pandas
df = pd.read_csv('amazon.csv')


# Mostrar las primeras 5 filas del dataset
df.head()

"""El siguiente paso es el Preprocesamiento: Limpiar y transformar los datos. Esto incluye manejar los valores faltantes, transformar tipos de datos, manejar duplicados..."""

# 1. Buscar filas duplicadas:
duplicate_rows = df[df.duplicated()]
print(f"Número de filas duplicadas: {len(duplicate_rows)}")

# 2. Buscar valores nulos en cada columna:
null_counts = df.isnull().sum()
print("\nValores nulos por columna:")
print(null_counts)

"""Como solo hay dos filas con valores nulos, vamos a verlas para decidir qué hacer con ellas:"""

null_rows = df[df.isnull().any(axis=1)]
print("Filas con valores nulos:")
print(null_rows)

"""Las filas parecen correctas, y el significado de la columna rating_count es
"Number of people who voted for the Amazon rating", así que entendemos que ese
NaN es realmente un 0, así que rellenamos el dataset

También hay que comprobar que los tipos de las columnas sean los correctos, o corregirlos:
"""

print(df.dtypes)

"""discounted_price, actual_price, discount_percentage, rating y rating_count contienen valores numéricos y deberían ser columnas numéricas. Para convertirlas hay que eliminar las comas con las que se han indicado los miles, el símbolo de porcentaje, y el símbolo que está en los precios (el de la rupia india)."""

print(df.loc[1279, ['product_name', 'rating', 'rating_count']])

# Convertir columnas a tipos numéricos, manejando puntos decimales, comas de miles y símbolos
# Reemplazamos el símbolo de la rupia india y las comas en las columnas de precio
df['discounted_price'] = df['discounted_price'].str.replace('₹', '').str.replace(',', '')
df['actual_price'] = df['actual_price'].str.replace('₹', '').str.replace(',', '')
# Convertimos las columnas de precio a numérico, especificando el punto como separador decimal si es necesario
df['discounted_price'] = pd.to_numeric(df['discounted_price'], errors='coerce')
df['actual_price'] = pd.to_numeric(df['actual_price'], errors='coerce')
# Eliminamos el símbolo de porcentaje y convertimos a numérico la columna de descuento
df['discount_percentage'] = pd.to_numeric(df['discount_percentage'].str.rstrip('%'), errors='coerce')
# Convertimos las columnas de rating y rating_count a numérico
df['rating'] = pd.to_numeric(df['rating'], errors='coerce')
df['rating_count'] = df['rating_count'].str.replace(',', '').str.strip()  # Eliminar comas y espacios en blanco
df['rating_count'] = pd.to_numeric(df['rating_count'], errors='coerce').astype('Int64')# Convertir a Int64
df['rating_count'] = df['rating_count'].fillna(0)
df['rating_count'] = df['rating_count'].infer_objects()

# Verificar los tipos de datos actualizados
print(df.dtypes)

"""Como la conversión ha podido tener errrores que le hemos dicho que rellene con NaN para que el programa no falle, vamos a comprobarlos:"""

# Verificar valores nulos después de la conversión
null_counts_after_conversion = df[['discounted_price', 'actual_price', 'discount_percentage', 'rating', 'rating_count']].isnull().sum()
print("\nValores nulos por columna (después de la conversión):")
print(null_counts_after_conversion)

# Crear un DataFrame con las filas que tienen NaN en 'rating' o 'rating_count'
error_rows = df[df[['rating', 'rating_count']].isnull().any(axis=1)]

# Mostrar las filas con errores
print("Filas con errores en 'rating' o 'rating_count':")
print(error_rows[['product_name', 'rating', 'rating_count']])

"""Vemos un error que aparece en Rating. Al estudiar de dónde proviene, vemos que es porque en el df original, en esa posición había un |. Aunque por similitud del símbolo podríamos pensar que es un 1, nos parece aventurado afirmarlo, y ya que es solamente una fila nos parece más seguro eliminarla del dataset que inventar el valor."""

# Eliminar filas con NaN en 'rating'
df = df.dropna(subset=['rating'])

"""El tercer paso de KDD es la Transformación de datos. En esta fase se reduce la dimensionalidad y se pueden crear nuevas características, además de codificar variables categóricas y normalizar o escalar las variables.

Vamos a empezar por reducir la dimensionalidad: en nuestro dasaset estamos cargando con varias columnas que no aportan al análisis:

img_link, product_link: Son enlaces web, sin valor .
user_name, product_name: son variables categóricas que podría requerir codificación numérica, y además da información redundante con user_id y product_id.
"""

# Eliminar columnas innecesarias
df = df.drop(['product_name', 'user_name', 'img_link', 'product_link'], axis=1)

"""La columna de Category es definitivamente interesante, pero habría que ver cuántos valores tiene y cómo podemos sacarles valor. Teniendo tantos valores distintos no es funcional hacer un onehot encoding y tratar cada uno por separado. Consideramos que puede ser suficiente el tomar la "categoría general", considerando solamente la primera de las categorías del árbol de categorías que tienen. Un estudio más concienzudo con todas las categorías sería posible, pero requeriría mucho más trabajo del que es viable en este trabajo."""

num_unique_categories = df['category'].nunique()
print(f"Número de valores distintos en 'category': {num_unique_categories}")

# PAra ver los valores únicos:
unique_categories = df['category'].unique()
print(f"\nValores únicos en 'category': {unique_categories}")

# Extraer la primera categoría
df['general_category'] = df['category'].str.split('|', expand=True)[0]

# Verificar la nueva columna
print(df[['category', 'general_category']].head())

num_unique_general_categories = df['general_category'].nunique()
print(f"Número de valores distintos en 'general_category': {num_unique_general_categories}")

# PAra ver los valores únicos:
unique_general_categories = df['general_category'].unique()
print(f"\nValores únicos en 'general_category': {unique_general_categories}")

"""Ya que la columna general_category que resume la información principal de la columna category, es recomendable eliminar la columna category original."""

df = df.drop('category', axis=1)

"""  La Id de producto y usuario podría sernos útil si alguna se repite: sería interesante agrupar las reseñas por producto para poder saber cómo son el conjunto de reseñas de ese producto. Respecto a los usuarios, podría ser interesante conocer usuarios que hagan muchas reseñas, especialmente si son muy votadas, ya que podría llegar a interesar enviarle productos para que sea "probador".
  Por cómo está hecho el dataset, las reseñas están hechas como "resumen o media de reseñas", en el texto es un resumen de las reseñas, y user_id y user_name tienen realmente una colección de ids y nombres. No podemos realmente separarlos: podríamos separarlos para saber quién hace muchas reseñas, pero no podemos conocer sus notas o su repercusión individual, de manera que, tal como está hecho el dataset, los ids no nos aportan información y vamos a eliminar también esas columnas
"""

# Eliminar columnas de id
df = df.drop(['user_id', 'product_id', 'review_id'], axis=1)

"""Las reviews son una fuente valiosa de información y deberían ser consideradas en la etapa de transformación de datos de KDD, ya que no pueden considerarse completas. Para ello vamos a intentar extraer el sentimiento de la reseña, si es positiva, negativa o neutral."""

nltk.download('vader_lexicon')
analyzer = SentimentIntensityAnalyzer()


# Combinar 'review_title' y 'review_content' en una nueva columna
df['combined_review'] = df['review_title'] + ' ' + df['review_content']


# Calcular el sentimiento de 'combined_review' utilizando VADER
df['overall_sentiment'] = df['combined_review'].apply(lambda review: analyzer.polarity_scores(review)['compound'])

# Categorizar el sentimiento general en positivo, negativo o neutral
df['overall_sentiment_category'] = pd.cut(df['overall_sentiment'], bins=[-1, -0.1, 0.1, 1], labels=['negative', 'neutral', 'positive'])

"""También vamos a obtener las palabras más relevantes por categoría y por sentimiento, para intentar averiguar a qué le dan importancia los clientes."""

nltk.download('punkt_tab')
nltk.download('punkt')
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

def get_frequent_words(reviews, n=10):
    import string  # Importar string dentro de la función
    all_tokens = []
    for review in reviews:
        # Eliminar signos de puntuación
        review = review.translate(str.maketrans('', '', string.punctuation))
        tokens = [token for token in word_tokenize(review) if token.lower() not in stop_words]
        all_tokens.extend(tokens)
    word_counts = Counter(all_tokens)
    return word_counts.most_common(n)

# Obtener las palabras más frecuentes para cada sentimiento
sentiment_words = df.groupby('overall_sentiment_category')['combined_review'].apply(get_frequent_words)

# Mostrar las palabras más frecuentes para cada sentimiento
for sentiment, words in sentiment_words.items():
    print(f"Sentimiento: {sentiment}")
    for word, count in words:
        print(f"  {word}: {count}")
    print("\n")

# Obtener las palabras más frecuentes para cada categoría general
category_words = df.groupby('general_category')['combined_review'].apply(get_frequent_words)

# Mostrar las palabras más frecuentes para cada categoría general
for category, words in category_words.items():
    print(f"Categoría: {category}")
    for word, count in words:
        print(f"  {word}: {count}")
    print("\n")

"""Respecto a los precios,  las columnas discounted_price, actual_price, y discount_percentage parecen tener toda la información que necesitamos. Podría ser interesante crear rangos de precios para simplificar la relación entre precios y reseñas, pero vamos a intentar no hacerlo. Lo que sí parece necesario hacer es normalizar los datos, para mejorar el análisis. Lo haremos con MinMaxScaler, ya que no tenemos outliers que alteren el resultado."""

# Crear un objeto MinMaxScaler
scaler = MinMaxScaler()

# Ajustar el scaler a las columnas de precios
scaler.fit(df[['discounted_price', 'actual_price']])

# Transformar las columnas de precios
df[['discounted_price_normalized', 'actual_price_normalized']] = scaler.transform(df[['discounted_price', 'actual_price']])

"""Con esto completamos la fase 3 de KDD. Recordamos:
Eliminación de columnas innecesarias (user_id, product_id, category, etc.).
Creación de nuevas características (general_category, overall_sentiment, overall_sentiment_category, combined_review, etc.).
Análisis de sentimiento de las reviews (title_sentiment, content_sentiment, overall_sentiment).
Extracción de palabras clave por sentimiento y categoría general (sentiment_words, category_words).
Normalización de los precios (discounted_price_normalized, actual_price_normalized).

Pasamos a la fase 4: Minería de datos.

Lo que queremos saber es qué factores influyen en la satisfacción del cliente, y cuándo este está satisfecho.
Primero vamos a ver cómo de relacionados están el sentimiento y el rating, para saber a partir de qué nota de un producto debería preocuparnos que tenga una valoración neutra o mala.
"""

correlation = df['rating'].corr(df['overall_sentiment'])
print(f"Correlación entre Rating y Sentimiento: {correlation}")

# Calcular la distribución del rating para cada categoría de sentimiento
rating_distribution = df.groupby('overall_sentiment_category', observed=False)['rating'].describe()

# Mostrar la distribución
print(rating_distribution)

# Crear un diagrama de caja del rating por categoría de sentimiento
sns.boxplot(x='overall_sentiment_category', y='rating', data=df)
plt.show()

"""Después de probar el análisis de sentimientos con VADER y con TextBlob (siendo algo más coherente el de VADER), vemos que extraer el sentimiento de las reseñas es complejo y que deberíamos dar más peso a la nota numérica, que debería reflejar mejor la opinión del cliente, que al sentimiento que estos sistemas han extraído del texto.

Para poner en un contexto mejor el análisis de las palabras, vamos a utilizar nubes de palabras, clasificándolas por las notas (malas, medias y buenas, ya que el análisis de sentimientos no ha mostrado tanta relevancia como pensábamos en principio), y por categorías. Esto nos debería permitir conocer a qué le dan más importancia los clientes, al aparecer más en las reseñas:
"""

!pip install wordcloud

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt

# Define los rangos de rating que deseas utilizar
rating_ranges = [(1, 2), (3, 4), (4.5, 5)]

# Crea una nube de palabras para cada rango de rating
for rating_range in rating_ranges:
    # Filtra las reviews dentro del rango de rating
    filtered_reviews = df[(df['rating'] >= rating_range[0]) & (df['rating'] <= rating_range[1])]

    # Combina las reviews en un solo texto
    text = " ".join(review for review in filtered_reviews['combined_review'])

    # Crea la nube de palabras
    wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=STOPWORDS).generate(text)

    # Muestra la nube de palabras
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"Nube de palabras para rating {rating_range[0]}-{rating_range[1]}")
    plt.show()

# Define las categorías generales que deseas utilizar
general_categories = df['general_category'].unique()

# Crea una nube de palabras para cada categoría general
for general_category in general_categories:
    # Filtra las reviews con la categoría general deseada
    filtered_reviews = df[df['general_category'] == general_category]

    # Combina las reviews en un solo texto
    text = " ".join(review for review in filtered_reviews['combined_review'])

    # Crea la nube de palabras
    wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=STOPWORDS).generate(text)

    # Muestra la nube de palabras
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"Nube de palabras para categoría {general_category}")
    plt.show()

"""Ahora nos preguntamos si existe alguna relación entre el precio y el descuento, y la nota. ¿Los productos con más descuentos o con precios más bajos tienen mejores notas? ¿Los clientes son más exigentes con los productos caros?"""

correlation = df['discount_percentage'].corr(df['rating'])
print(f"Correlación entre descuento y rating: {correlation}")

sns.scatterplot(x='discount_percentage', y='rating', hue='general_category', data=df)
plt.show()

import statsmodels.formula.api as sm
model = sm.ols(formula="rating ~ discount_percentage", data=df).fit()
print(model.summary())

"""Un primer estudio rápido de la correlación del porcentaje de descuento y el rating, y una búsqueda de relación lineal con statsmodels parecen indicar que no existe tal relación lineal. Vamos a ver si una red neuronal puede encontrar las relaciones y predecir el rating: Ya que entendemos que las variables que probablemente interfieran más en el rating sean discount_percentage, actual_price, general_category, y rating_count, ponemos un imput_shape de 4"""

import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split

# Define las columnas de entrada
numeric_features = ['discount_percentage', 'actual_price', 'rating_count']
categorical_features = ['general_category']

# Crea un pipeline para preprocesar las variables numéricas
numeric_transformer = Pipeline(steps=[
    ('scaler', MinMaxScaler())
])

# Crea un pipeline para preprocesar las variables categóricas
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))
])

# Combina los pipelines utilizando ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Define el modelo de red neuronal
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(preprocessor.fit_transform(df).shape[1],)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1)
])

# Compila el modelo
model.compile(loss='mse', optimizer='adam', metrics=['mae'])

# Ajusta el preprocesador a los datos y transforma las características
X = preprocessor.fit_transform(df)
y = df['rating']

# Divide los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Entrena el modelo
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1)

# Evalua el modelo
loss, mae = model.evaluate(X_test, y_test, verbose=0)
print(f'Loss: {loss:.4f}, MAE: {mae:.4f}')

# Realiza predicciones en el conjunto de prueba
y_pred = model.predict(X_test)

# Realiza predicciones en el conjunto de prueba
y_pred = model.predict(X_test)

# Imprimir las primeras 5 predicciones y los valores reales correspondientes
for i in range(5):
    print(f"Predicción: {y_pred[i][0]:.2f}, Valor real: {y_test.iloc[i]:.2f}")

from sklearn.metrics import mean_squared_error, mean_absolute_error

mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

print(f"Error cuadrático medio (MSE): {mse:.4f}")
print(f"Error absoluto medio (MAE): {mae:.4f}")

errors = y_pred.flatten() - y_test  # Calcula los errores
plt.hist(errors, bins=20)  # Ajusta el número de bins según tus necesidades
plt.xlabel("Error")
plt.ylabel("Frecuencia")
plt.title("Histograma de los errores")
plt.show()

plt.scatter(y_test, y_pred.flatten(), alpha=0.3)  # Ajusta alpha para controlar la transparencia
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red')
plt.xlabel("Valor real")
plt.ylabel("Predicción")
plt.title("Comparación entre predicciones y valores reales")
plt.show()

"""El modelo hace unas predicciones bastante razonables, lo que podría indicar que hemos elegido bien las variables de entrada. Vamos a ver qué peso tiene cada una de ellas,para poder comprobarlo:"""

# Obtén los pesos de la primera capa densa
weights = model.layers[0].get_weights()[0]

# Obtén los nombres de las características de entrada
feature_names = numeric_features + list(preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features))

# Crea un gráfico de barras
plt.bar(feature_names, np.mean(weights, axis=1)) # Promedia los pesos para cada característica
plt.xticks(rotation=90) # Rota las etiquetas del eje x para una mejor legibilidad
plt.xlabel("Característica")
plt.ylabel("Peso promedio")
plt.title("Pesos de la primera capa densa")
plt.show()

from sklearn.cluster import KMeans

# Selección de las variables para el clustering
X = df[['rating', 'discounted_price_normalized', 'actual_price_normalized', 'discount_percentage']]

# Crea un modelo de K-means con un número determinado de clusters
kmeans = KMeans(n_clusters=3, random_state=42)

# Ajusta el modelo a los datos
kmeans.fit(X)

# Asigna los clusters a los productos
df['cluster'] = kmeans.labels_

sns.pairplot(df[['rating', 'discounted_price_normalized','actual_price_normalized', 'discount_percentage', 'cluster']], hue='cluster')
plt.show()

# Calcula las estadísticas descriptivas para cada cluster
cluster_stats = df.groupby('cluster')[['rating','discounted_price', 'actual_price', 'discount_percentage']].describe()

# Imprime las estadísticas descriptivas
pd.set_option('display.max_columns', None)  # Muestra todas las columnas
pd.set_option('display.width', None)       # Ajusta el ancho de la salida para mostrar todas las columnas

print(cluster_stats)

#Evaluación de los clústeres
from sklearn.metrics import silhouette_score, davies_bouldin_score

# Calcula el índice de silueta
silhouette = silhouette_score(X, df['cluster'])
print(f"Índice de silueta: {silhouette}")

# Calcula el índice de Davies-Bouldin
davies_bouldin = davies_bouldin_score(X, df['cluster'])
print(f"Índice de Davies-Bouldin: {davies_bouldin}")

"""Con esto terminamos la fase de minería de datos y pasamos a la última fase de KDD, la Evaluación.

Como era de esperar, en las reseñas negativas destaca "bad quality", y "return": los clientes no están dispuestos a quedarse con productos que no les gustan y quieren devolverlos. Sí llama la atención que las palabras de reseñas de notas medias y altas son similares. Eso, unido al pequeño número de reseñas de valores medios, podría indicar que en realidad los clientes tienden a valorar con la nota máxima o mínima según si el producto les gusta o no, y solamente los pocos clientes que dan mucha importancia a las reseñas se preocupan de pensar en una nota para el producto, por lo que una nota de 3,5 o 4 sobre 5 en realidad es una nota bastante buena. En cualquier caso destaca la palabra "Good", que respalda la búsqueda de calidad en los productos; y "price", lo que indica la importancia del precio a la hora de evaluar un producto.

Respecto a las categorías, este análsis permite identificar algunos de los productos más reseñados y algunas de las características que más interesan a los clientes. En casi todos, además de Good y Price, aparecen Quality, working... Lo que indica que ante todo los clientes valoras la calidad y que los productos funcionen como se espera. Aparte de eso, destacan.

Computers & Accessories:
Cable y charging: Parecen indicar la importancia de que los dispositivos tengan una carga rápida.
USB: Relacionado con la conectividad y compatibilidad de los dispositivos.
Durability: La durabilidad y la resistencia del producto también son valoradas.

Electronics:
Camera y Sound quality: Parecen ser las dos características que más se valoran en estos dispositivos.
Battery: La duración de la batería es un factor importante para los dispositivos electrónicos.
Problem: Parece que este tipo de productos presentan más problemas que otras categorías.
Screen: La calidad de la pantalla es relevante en dispositivos con pantallas.

Instruments:
Guitar: Indica la relevancia de productos para guitarras.
Quality Sound: La calidad del sonido y del instrumento es fundamental.
Easy: La facilidad de uso del instrumento es valorada.
Mic y noise reduction: Indican que la calidad del micrófono y la capacidad de reducir ruido son importantes.

Home & Kitchen:
Son más genericas. Quality, Good, Product y Easy Use indican que la calidad general y la facilidad de uso son lo que se busca.

Clothing & Accessories:
Quality y Good: La calidad de la ropa y los accesorios es un factor clave.
Size y Fit: El tamaño y el ajuste cuadren con lo esperado con la talla son importantes en la ropa.
Look: El aspecto y el estilo de la ropa son valorados.

Books:
Good y Story: La trama y la narrativa son importantes en los libros.
Time: Podría indicar la duración o el tiempo dedicado a la lectura, o el tiempo de entrega.

En análisis de los pesos de la red neuronal nos indica que lo más importante en la nota de un producto, de las características que hemos estudiado, es su categoría.  La mayor relevancia de general_category sugiere que la categoría del producto es un factor importante para predecir su rating. Esto tiene sentido, ya que diferentes categorías de productos pueden tener diferentes expectativas de calidad o satisfacción del cliente.
La influencia de actual_price podría indicar que el precio del producto tiene un impacto en la percepción de su calidad o valor por parte de los clientes. Es posible que los productos más caros se perciban como de mayor calidad, o que los clientes sean más exigentes con los productos de mayor precio, y por eso tiene también un alto peso.
La popularidad del producto, medida en el rating_count, en las veces que ha sido reseñado, en cambio, muestra una muy baja influencia en la predicción de la nota.

Por último, la segmentación de clientes en clústeres nos ha permitido identificar tres tipos de clientes:
Cluster 0 (Cazadores de ofertas): Este cluster representa a los clientes que buscan activamente productos con descuentos significativos. No son necesariamente los que compran los productos más baratos, pero sí buscan un buen ahorro en sus compras. Podrían ser sensibles al precio y estar dispuestos a esperar a que un producto tenga un descuento antes de comprarlo.

Cluster 1 (Clientes Premium): Este cluster representa a los clientes que están dispuestos a pagar precios más altos por los productos y no son tan sensibles a los descuentos. Podrían valorar más la calidad, la marca o la exclusividad que el precio en sí. No suelen buscar activamente descuentos y están dispuestos a pagar el precio completo por un producto que les guste.

Cluster 2 (Cazadores de gangas):  Este cluster representa a los clientes que buscan los productos más baratos y con los mayores descuentos posibles. Son muy sensibles al precio y priorizan el ahorro por encima de otras consideraciones. Podrían estar dispuestos a sacrificar la calidad o la marca por un precio más bajo. Suelen buscar activamente ofertas y promociones.

Curiosamente, los ratings son independientes de los clusteres, lo que refuerza la idea que los clientes hacen sus valoraciones teniendo en cuenta lo que esperan del producto y puntúan si les gusta o no, no hacen una valoración numérica utilizando una escala, comparando el producto con otros.

Basado en estos resultados, podemos pensar en algunas estrategias de producto que podrían mejorar los resultados:

*   Énfasis en la calidad: Dado que la calidad es una característica importante para los clientes, Amazon debería asegurarse de ofrecer productos de alta calidad en todas las categorías.

*   Amplitud de catálogo: Ofrecer una amplia variedad de productos en diferentes categorías para satisfacer las diversas preferencias de los clientes.

*   Productos premium: Considerar la posibilidad de ofrecer una línea de productos premium para el cluster de clientes menos sensibles al precio y al descuento.

También surgen algunas ideas para las campañas de márketing:


*   Marketing personalizado: Dirigir campañas de marketing específicas a cada cluster de clientes, teniendo en cuenta sus preferencias de precio y descuento.


*   Promociones segmentadas: Ofrecer descuentos y promociones especiales a los clusters de clientes más sensibles al precio.

*   Comunicación efectiva: Resaltar las características de los productos que son importantes para los clientes en las descripciones y en las comunicaciones de marketing.

Con estas medidas debería poder observarse un aumento en las ventas y en la satisfacción de los clientes, reflejada en un aumento en los rating de los productos.
"""
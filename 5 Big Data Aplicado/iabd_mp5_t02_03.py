# -*- coding: utf-8 -*-
"""IABD_MP5_T02_03.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gzTnaD0wdFqIDtaNAkg0oKTado-VXx6q

Crea un programa en Python usando el paradigma MapReduce que sirva para contar el número de ocurrencias de cada palabra en un conjunto de documentos.
El programa debe de contar con las siguientes fases:
1.	Fase Map:
o	Leer cada documento línea por línea.
o	Dividir cada línea en palabras.
o	Emitir un par clave-valor para cada palabra, donde la clave es la palabra y el valor es 1.
2.	Fase Shuffle and Sort:
o	Agrupar todas las palabras iguales juntas.
3.	Fase Reduce:
o	Sumar todos los valores para cada palabra.

Para que el programa sea interactivo y permita al usuario introducir los textos importo ipywidgets para usar un widget de entrada de texto, como un Textarea, para que el usuario pueda escribir o pegar los documentos. Luego, podemos procesar el texto introducido para la fase Map.  Informo al usuario de la cantidad de documentos procesados
"""

import ipywidgets as widgets
from IPython.display import display

text_area = widgets.Textarea(
    placeholder='Introduce los documentos aquí, separados por una línea en blanco',
    layout=widgets.Layout(width='500px', height='200px')
)

display(text_area)

def procesar_textos(button):
    documents = text_area.value.split('\n\n')  # Dividir por dos saltos de línea consecutivos
    num_documents = len(documents)  # Contar el número de documentos
    print(f"Documentos procesados: {num_documents}")

    # Fase Map
    """
    En la fase map hago que el programa separe cada documento por los saltos de
    línea en "lines", y después que separe cada line por palabras. Como está
    contando todas las palabras, las almaceno en una lista, cada palabra con un
    valor de 1.
    """
    word_list = [] # Aquí se almacenarán los pares clave-valor
    for doc_index, document in enumerate(documents): # para cada documento
        for line in document.split('\n'): #separa cada linea del documento
            for word in line.split(): # separa cada palabra de la línea
                # Para comprobar cada palabra y a qué documento pertenece
                # print(f"Documento {doc_index}: Palabra '{word}'")
                word_list.append((word, 1)) # Emitir par clave-valor (word, 1)


    # Fase Shuffle and Sort
    """
    En la fase Shuffle creo un diccionario para guardar cada palabra junto
    con el número de veces que se repite en los documentos. Para ello, recorre
    la lista word_list de la fase Map y bien la incluye con el número 1 si es
    nueva o aumenta la cuenta si ya existía
    """
    word_counts = {}
    for word, count in word_list:
        if word in word_counts:
            word_counts[word].append(count)  # Agregar el conteo a la lista de la palabra
        else:
            word_counts[word] = [count]  # Inicializar la cuenta
    #print("Palabras y cuenta:", word_counts)

    # Fase Reduce
    """
    En la fase reduce creo el diccionario final con la frecuencia de las palabras
    Para ello, recorre el diccioario word_counts y suma los conteos de cada palabra.
    Como el resultado ya es el final, para hacerlo más legible e informativo
    lo imprimo por orden de frecuencia y por orden alfabético, que lo encuentro
    más útil que por orden de aparición en el documento.
    """
    word_frequencies = {}  # Diccionario para almacenar las frecuencias finales
    for word, counts in word_counts.items():
        word_frequencies[word] = sum(counts)  # Sumar los conteos de cada palabra

    # Imprimir las frecuencias de palabras ordenadas por frecuencia y luego alfabéticamente
    print("Frecuencias de palabras (ordenadas por frecuencia y luego alfabéticamente):")
    for word, frequency in sorted(word_frequencies.items(), key=lambda item: (-item[1], item[0])):
        print(f"{word}: {frequency}")


#Por último, el botón que hace funcionar el widget de entrada de texto

button = widgets.Button(description="Procesar Textos")
button.on_click(procesar_textos)
display(button)
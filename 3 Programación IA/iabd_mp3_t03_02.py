# -*- coding: utf-8 -*-
"""IABD_MP3_T03_02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BL4xfZoJHH1gSvZxTojRmQzXX2ZNlUSL

•	Tarea: Construye una red neuronal convolucional (CNN) usando PyTorch para clasificar las imágenes del dataset MNIST.

•	Requisitos:

o	Define la arquitectura de la CNN con capas convolucionales, capas de pooling y capas totalmente conectadas.

o	Usa un optimizador (por ejemplo, Adam) y una función de pérdida (por ejemplo, Cross-Entropy) para entrenar el modelo.

o	Visualiza las características aprendidas por las capas convolucionales utilizando una herramienta como matplotlib.

o	Evalúa el rendimiento del modelo usando métricas como la precisión (accuracy) y la matriz de confusión.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report

import seaborn as sns

"""Con el dataset cargado definimos la red.
La red tiene tres tipos de capas:

nn.Conv2d: Capas convolucionales para extraer características.

nn.MaxPool2d: Capas de Max Pooling para reducir la dimensionalidad.

nn.Linear: Capas totalmente conectadas para la clasificación.

forward: Define cómo fluyen los datos a través de la red: F.relu: Función de activación ReLU para introducir no linealidad.
x.view: Aplana la salida de las capas convolucionales para la entrada a la capa totalmente conectada.

Los parámetros que hemos utilizado se pueden modificar según el tipo de problema.

Parámetros de las capas convolucionales (Conv2d):

in_channels: Número de canales de entrada. En el caso de MNIST, las imágenes son en escala de grises, por lo que in_channels=1. Si fueran imágenes RGB, sería in_channels=3.

out_channels: Número de filtros o kernels que se aplicarán. En la primera capa (conv1), se usan 32 filtros, y en la segunda (conv2), 64. Un mayor número de filtros permite detectar más características, pero también aumenta la complejidad del modelo.

kernel_size: Tamaño del kernel o filtro convolucional. En este caso, se usa un kernel de 3x3, que es un tamaño común en CNNs. Un kernel más grande puede capturar características más globales, mientras que uno más pequeño se enfoca en detalles locales.

padding: Número de píxeles de relleno que se añaden alrededor de la imagen. En este caso, padding=1 agrega un píxel de relleno en cada lado. El padding ayuda a evitar que la información de los bordes se pierda durante la convolución.


"""

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1) # Capa convolucional 1
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)    # Capa de Max Pooling 1
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # Capa convolucional 2
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)    # Capa de Max Pooling 2
        self.fc1 = nn.Linear(7 * 7 * 64, 128)              # Capa totalmente conectada 1
        self.fc2 = nn.Linear(128, 10)                        # Capa totalmente conectada 2 (salida)

    def forward(self, x):
        x = F.relu(self.conv1(x))           # Activación ReLU después de la convolución 1
        x = self.pool1(x)                  # Max Pooling 1
        x = F.relu(self.conv2(x))           # Activación ReLU después de la convolución 2
        x = self.pool2(x)                  # Max Pooling 2
        x = x.view(-1, 7 * 7 * 64)         # Aplanar la salida para la capa totalmente conectada
        x = F.relu(self.fc1(x))           # Activación ReLU después de la capa totalmente conectada 1
        x = self.fc2(x)                    # Salida de la capa totalmente conectada 2
        return x

# Crear una instancia del modelo
model = CNN()

# Hiperparámetros
num_epochs = 10
batch_size = 64
learning_rate = 0.001

"""Definimos el optimizador y la función de pérdida.

Como función de pérdida tiene sentido usar Cross-Entropy porque se basa en la idea de minimizar la divergencia entre la distribución de probabilidad predicha por el modelo y la distribución real de las etiquetas. En clasificación multiclase, queremos que el modelo asigne una alta probabilidad a la clase correcta y una baja probabilidad a las demás clases. Cross-Entropy penaliza fuertemente las predicciones incorrectas, especialmente cuando el modelo asigna una alta probabilidad a la clase incorrecta.

Como optimizadro usamos Adam, que es una buena opción por su eficiencia, su rápida convergencia y su capacidad para manejar gradientes ruidosos y dispersos
"""

# Definir la función de pérdida y el optimizador
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

"""Cargamos el dataset de MNIST. A la hora de cargar el dataset necesitamos hacerlo con torchvision para que sea compatible con PyTorch"""

# Transformaciones para las imágenes
transform = transforms.Compose([
    transforms.ToTensor(),  # Convertir a tensor de PyTorch
    transforms.Normalize((0.1307,), (0.3081,))  # Normalizar los datos
])

# Carga del dataset MNIST con torchvision
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

# Crear DataLoaders
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

"""Con todo preparado, entrenamos a la red CNN"""

# Define el dispositivo (CPU o GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Bucle de entrenamiento
for epoch in range(num_epochs):
    for batch_idx, (images, labels) in enumerate(train_loader):
        # Mueve los datos al dispositivo seleccionado
        images, labels = images.to(device), labels.to(device)

        # Reinicia los gradientes
        optimizer.zero_grad()

        # Realiza una predicción
        outputs = model(images)

        # Calcula la pérdida
        loss = criterion(outputs, labels)

        # Retropropagación
        loss.backward()

        # Actualiza los pesos
        optimizer.step()

        # Imprime el progreso (opcional)
        if batch_idx % 100 == 0:
            print(f"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}")

"""Usamos Matplotlib para mostrar los filtros de cada capa:"""

# Función para visualizar los filtros de una capa convolucional
def visualize_filters(layer, num_filters=8):

    # Obtiene los filtros de la capa y los convierte a NumPy.
    filters = layer.weight.data.cpu().numpy()

    # Crea una figura con subplots para mostrar los filtros.
    fig, axes = plt.subplots(1, num_filters, figsize=(20, 5))

    # Itera sobre los filtros y los muestra en los subplots.
    for i in range(num_filters):
        # Obtiene el filtro actual.
        filter = filters[i, 0, :, :]

        # Normaliza el filtro para visualizarlo.
        filter = (filter - filter.min()) / (filter.max() - filter.min())

        # Muestra el filtro en el subplot.
        axes[i].imshow(filter, cmap='gray')
        axes[i].axis('off')
        axes[i].set_title(f'Filtro {i+1}')

    # Ajusta la figura y la muestra.
    plt.tight_layout()
    plt.show()


# Visualizar los filtros de la primera capa convolucional (conv1)
visualize_filters(model.conv1)

# Visualizar los filtros de la segunda capa convolucional (conv2)
visualize_filters(model.conv2)

"""Por último evaluamos el modelo. De manera similar al anterior ejercicio, vamos a hacer el accuracy, la matriz de confusión y el classification report para tener los datos por clase."""

# Evaluación del modelo
model.eval()  # Pon el modelo en modo de evaluación
total_correct = 0
total_samples = 0
all_predictions = []
all_labels = []

with torch.no_grad():  # Desactiva el cálculo de gradientes durante la evaluación
    for images, labels in test_loader:
        # Movemos las imágenes y etiquetas al dispositivo (CPU o GPU)
        images, labels = images.to(device), labels.to(device)

        # Obtenemos las salidas del modelo
        outputs = model(images)

        # Obtenemos las predicciones (índices de las clases con mayor probabilidad)
        _, predictions = torch.max(outputs, 1)

        # Actualizamos el conteo de predicciones correctas
        total_correct += (predictions == labels).sum().item()

        # Actualizamos el conteo total de ejemplos
        total_samples += labels.size(0)

        # Almacenamos las predicciones y etiquetas para calcular métricas
        all_predictions.extend(predictions.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Calcula la precisión (accuracy)
accuracy = total_correct / total_samples
print(f'Precisión (Accuracy): {accuracy:.4f}')

# Calcula la matriz de confusión
cm = confusion_matrix(all_labels, all_predictions)
print('Matriz de Confusión:')
# Visualizar la matriz de confusión con seaborn
plt.figure(figsize=(10, 8))  # Ajusta el tamaño de la figura
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')  # Crea el mapa de calor
plt.title('Matriz de Confusión')
plt.xlabel('Predicciones')
plt.ylabel('Etiquetas reales')
plt.show()

# Calcula la precisión, recall y F1 por clase
report = classification_report(all_labels, all_predictions, output_dict=False)
print('Informe de Clasificación:')
print(report)